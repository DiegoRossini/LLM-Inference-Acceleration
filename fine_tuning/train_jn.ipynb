{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38edbe89-c192-47c8-98bc-9a6688592bc0",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996da930-9c19-4308-b5b0-ba7a93d66eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "from dataclasses import dataclass, field\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4353a3-1761-462d-ba6a-22b67becd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n",
    "    )\n",
    "    tokenizer_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to pretrained tokenizer or tokenizer identifier from huggingface.co/models\"},\n",
    "    )\n",
    "    adapter_paths: List[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"A list of paths to multi model adapters.\"},\n",
    "    )\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Max length of the tokenizer\"},\n",
    "    )\n",
    "    huggingface_token: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Huggingface token for private model\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use fast tokenizer\"},\n",
    "    )\n",
    "    output_dir: str = field(\n",
    "        default=\"./\",\n",
    "        metadata={\"help\": \"Output directory for the model\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc1cb376-b6e6-4d6c-a9cf-cf9872b9ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataArguments:\n",
    "    train_file: str = field(\n",
    "        default=None, \n",
    "        metadata={\"help\": \"Path to the train data in jsonl format.\"}\n",
    "    )\n",
    "    dataset_text_field: Optional[str] = field(\n",
    "        default=\"instruction\", \n",
    "        metadata={\"help\": \"The field in dataset for completion pretraining. Mandatory if 'pre_sft' is True.\"}\n",
    "    )\n",
    "    user_prompt_format: Optional[str] = field(\n",
    "        default=\"llama2\",\n",
    "        metadata={\"help\": \"The name of a known llm model prompt format or a the user custom prompt. Mandatory if 'instruct_sft' is True.\"}\n",
    "    )\n",
    "    user_response_sentence: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The response sentence for instruction pretraining\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1700622-2be5-47fe-a3fb-39a42d9a6347",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingArguments:\n",
    "    pre_sft: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to pretrain the model\"},\n",
    "    )\n",
    "    instruct_sft: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to instruct the model\"},\n",
    "    )\n",
    "    packing: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to pack the data into constant length batches to accelerate training\"},\n",
    "    )\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False\n",
    "    num_epochs: int = 3\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 5e-5\n",
    "    optim: str = \"adamw_apex_fused\"\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    logging_strategy: str = \"steps\"\n",
    "    logging_steps: int = 100\n",
    "    logging_dir: str = \"/app/logging_finetuning/\",\n",
    "    save_strategy: str = \"no\"\n",
    "    save_steps: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a3bf1c2-285f-4e9e-bcef-40ed640bcdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AdapterArguments:\n",
    "    # TODO: How to get the optimal paratmeters for lora?\n",
    "    use_lora: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use lora\"},\n",
    "    )\n",
    "    lora_target_modules: List[str] = field(\n",
    "        default_factory=lambda:[\"qkv_proj\"],\n",
    "        metadata={\"help\": \"Target modules for lora\"},\n",
    "    )\n",
    "    lora_r: int = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"R for lora\"},\n",
    "    )\n",
    "    lora_alpha: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"Alpha for lora\"},\n",
    "    )\n",
    "    lora_dropout: float = field(\n",
    "        default=0.05,\n",
    "        metadata={\"help\": \"Dropout for lora\"},\n",
    "    )\n",
    "    lora_bias: str = field(\n",
    "        default=\"none\",\n",
    "        metadata={\"help\": \"Bias for lora\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ed291-792a-4d0d-b9c8-ddf938b9fe25",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f34d92-691c-466f-a6e8-541fc1751b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "from params import (\n",
    "    ModelArguments, \n",
    "    DataArguments, \n",
    "    TrainingArguments, \n",
    "    AdapterArguments\n",
    ")\n",
    "from prompt_template import PromptFormatter\n",
    "from callbacks import SavingCallback, SavingCallbackHalf\n",
    "from utils import merge_peft_model\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7dc6d6-9a23-4665-a41d-c240b19d748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"../../home/drossini/models/phi_mean_pooled/\",\n",
    "    tokenizer_name_or_path=\"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    adapter_paths=None,\n",
    "    model_max_length=2048,\n",
    "    huggingface_token=\"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\",\n",
    "    use_fast_tokenizer=True,\n",
    "    output_dir=\"../../home/drossini/models/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0688564-1cc1-47c3-9249-5f385a98fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = DataArguments(\n",
    "    train_file=\"../SlimPajama_chunk/slim_pajama_chunk_of_chunk.jsonl\",\n",
    "    dataset_text_field=\"text\",\n",
    "    user_prompt_format=None,\n",
    "    user_response_sentence=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2203be-1b41-4514-b3f1-ddcdaf4855cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    pre_sft=True,\n",
    "    instruct_sft=False,\n",
    "    packing=True,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    num_epochs=3,\n",
    "    batch_size=2,\n",
    "    learning_rate=3e-5,\n",
    "    optim=\"adamw_hf\",\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    logging_dir=\"../Fine_tuning/pre_sft_logging_finetuning/\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8cde46-e4f7-4379-a786-7a3a0d0de7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_args = AdapterArguments(\n",
    "    use_lora=True,\n",
    "    lora_target_modules=[\"qkv_proj\"],  # Set the target module to qkv_proj\n",
    "    lora_r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    lora_bias=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5a02be0-bc84-47d5-a6da-887d15dc5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_args(\n",
    "    model_args: ModelArguments, \n",
    "    data_args: DataArguments,\n",
    "    training_args: TrainingArguments,\n",
    "    adapter_args: AdapterArguments,\n",
    "    args: Dict\n",
    "):\n",
    "    return transformers.TrainingArguments(\n",
    "        output_dir=model_args.output_dir,\n",
    "        fp16=training_args.fp16,\n",
    "        bf16=training_args.bf16,\n",
    "        num_train_epochs=training_args.num_epochs,\n",
    "        per_device_train_batch_size=training_args.batch_size,\n",
    "        learning_rate=training_args.learning_rate,\n",
    "        optim=training_args.optim,\n",
    "        gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "        logging_dir=training_args.logging_dir,\n",
    "        logging_strategy=training_args.logging_strategy,\n",
    "        logging_steps=training_args.logging_steps,\n",
    "        save_strategy=training_args.save_strategy,\n",
    "        save_steps=training_args.save_steps,\n",
    "        **args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa4da07-5366-476f-a166-79fc066fa1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clm(\n",
    "    model_args: ModelArguments, \n",
    "    data_args: DataArguments,\n",
    "    training_args: TrainingArguments,\n",
    "    adapter_args: AdapterArguments\n",
    "    ):\n",
    "\n",
    "    args = {}\n",
    "\n",
    "    # huggingface login\n",
    "    if model_args.huggingface_token:\n",
    "        huggingface_hub.login(token=model_args.huggingface_token)\n",
    "\n",
    "    # load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # apply adapter\n",
    "    if model_args.adapter_paths:\n",
    "        model = merge_peft_model(model, model_args.adapter_paths)\n",
    "\n",
    "    lora_config = None\n",
    "    if adapter_args.use_lora:\n",
    "        lora_config = LoraConfig(\n",
    "            r = adapter_args.lora_r,\n",
    "            lora_alpha = adapter_args.lora_alpha,\n",
    "            lora_dropout = adapter_args.lora_dropout,\n",
    "            target_modules = adapter_args.lora_target_modules,\n",
    "            bias= adapter_args.lora_bias\n",
    "        )\n",
    "        args[\"remove_unused_columns\"] = False\n",
    "    \n",
    "    # load tokenizer\n",
    "    tokenizer_name_or_path = model_args.model_name_or_path\n",
    "    if model_args.tokenizer_name_or_path:\n",
    "        tokenizer_name_or_path = model_args.tokenizer_name_or_path\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        trust_remote_code=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # set TrainingArguments for huggingface trainer\n",
    "    args = prepare_args(model_args, data_args, training_args, adapter_args, args)\n",
    "    # completion pretraining\n",
    "    if training_args.pre_sft:\n",
    "        dataset = load_dataset('json', data_files=data_args.train_file, split=\"train\")\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            peft_config=lora_config,\n",
    "            train_dataset=dataset,\n",
    "            dataset_text_field=data_args.dataset_text_field,\n",
    "            packing=training_args.packing,\n",
    "            max_seq_length=model_args.model_max_length,\n",
    "            args=args,\n",
    "            callbacks=[SavingCallback() if adapter_args.use_lora else SavingCallbackHalf()]\n",
    "        )\n",
    "    # instruction pretraining\n",
    "    elif training_args.instruct_sft:\n",
    "        dataset = load_dataset('json', data_files=data_args.train_file, split=\"train\")\n",
    "\n",
    "        # format to correct prompt template\n",
    "        prompt_formatter = PromptFormatter(data_args.user_prompt_format)\n",
    "        response_template = data_args.user_response_sentence\n",
    "        if response_template is None:\n",
    "            response_template = prompt_formatter.response_template\n",
    "\n",
    "        collator = DataCollatorForCompletionOnlyLM(\n",
    "            response_template=response_template, \n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=dataset,\n",
    "            peft_config=lora_config,\n",
    "            max_seq_length=model_args.model_max_length,\n",
    "            args=args,\n",
    "            formatting_func=prompt_formatter.formatting_train_prompts_func,\n",
    "            data_collator=collator,\n",
    "            callbacks=[SavingCallback() if adapter_args.use_lora else SavingCallbackHalf()]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Please specify the training mode: pre_sft or instruct_sft\")\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "288a9bc1-c9bf-4a12-9f8a-0f0879af2fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6950b32-11ae-42d5-a6bb-be72c2feaa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/drossini/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcacbe462a314726b03f4d2c7f78edca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drossini/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, packing, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/drossini/.local/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/drossini/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/drossini/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/drossini/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9548853f46af45239d7a0774825487eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/drossini/.local/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='20610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  180/20610 02:42 < 5:11:17, 1.09 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_clm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 95\u001b[0m, in \u001b[0;36mtrain_clm\u001b[0;34m(model_args, data_args, training_args, adapter_args)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify the training mode: pre_sft or instruct_sft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:440\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 440\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py:2121\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2121\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_clm(model_args, data_args, training_args, adapter_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262ca60-6f65-4669-ba96-93b537fc982e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
