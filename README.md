# LLM-Inference-Acceleration
Enhancing performance by replacing Multihead Attention with Grouped Query Attention
