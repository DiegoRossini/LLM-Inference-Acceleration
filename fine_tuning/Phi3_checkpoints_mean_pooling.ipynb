{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d938ba4-fa55-4f94-b71b-0e1bfb998b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/drossini/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Authenticates with the Hugging Face Hub using provided API key\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "login(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd17ab03-3c40-4263-af4e-f0b8fdea3485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Loading the model to test the answers with the dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, pipeline\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "mha_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0242bb81-3c4d-483f-88b9-dc38b78baa8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac928f69-ea0e-4503-9377-8b07a3d6c38d",
   "metadata": {},
   "source": [
    "**Try**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8f62f9-a6e0-4a0d-86fe-5735f2e282b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions_wts = mha_model.state_dict().copy()\n",
    "num_heads = 16\n",
    "gqa_groups = num_heads // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a91c40d-7c23-48fd-9b95-c52a7e5c5f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_attention_to_heads(input_tensor, num_splits):\n",
    "    # Get the shape of the input tensor\n",
    "    rows, cols = input_tensor.shape\n",
    "\n",
    "    # Check if the number of rows is divisible by the number of splits\n",
    "    if rows % num_splits != 0:\n",
    "        raise ValueError(\"Number of rows is not divisible by the number of splits\")\n",
    "\n",
    "    # Calculate the number of rows in each split\n",
    "\n",
    "    # Use chunk to split the tensor along the rows\n",
    "    split_tensors = input_tensor.chunk(num_splits, dim=0)\n",
    "\n",
    "    return split_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ac8f59e-3a7f-4348-8b4b-b36726f95fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_heads(tensor_tuple, group_size, dtype):\n",
    "    # Initialize an empty list to store the averaged tensors\n",
    "    averaged_tensors = []\n",
    "\n",
    "    # Iterate through the tuple and average consecutive groups\n",
    "    for i in range(0, len(tensor_tuple), group_size):\n",
    "        # Take a group of tensors\n",
    "        tensor_group = tensor_tuple[i:i + group_size]\n",
    "\n",
    "        # Calculate the mean along dimension 0\n",
    "        averaged_tensor = torch.mean(torch.stack(tensor_group), dim=0, dtype=dtype)\n",
    "\n",
    "        # Append the averaged tensor to the list\n",
    "        averaged_tensors.append(averaged_tensor)\n",
    "\n",
    "    # Convert the list of averaged tensors to a tuple\n",
    "    averaged_tensors_tuple = tuple(averaged_tensors)\n",
    "\n",
    "    return averaged_tensors_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82707e2e-897b-4b64-a30d-fb9542414d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process the weights for no Phi models\n",
    "\n",
    "# for name_wts in list(attentions_wts.keys()):\n",
    "#     if len(attentions_wts[name_wts].shape) >= 2:\n",
    "#         tensor_to_process = attentions_wts[name_wts].clone()\n",
    "#         torch_dtype = tensor_to_process.dtype\n",
    "        \n",
    "#         # Process k_proj weights\n",
    "#         if \"k_proj\" in name_wts:\n",
    "#             attn_heads = split_attention_to_heads(tensor_to_process, num_splits=num_heads)\n",
    "#             gqa_tensors_grouped = average_heads(attn_heads, gqa_groups, dtype=torch_dtype)\n",
    "#             new_key = torch.cat(gqa_tensors_grouped)\n",
    "#             attentions_wts[name_wts] = new_key\n",
    "        \n",
    "#         # Process v_proj weights\n",
    "#         elif \"v_proj\" in name_wts:\n",
    "#             attn_heads = split_attention_to_heads(tensor_to_process, num_splits=num_heads)\n",
    "#             gqa_tensors_grouped = average_heads(attn_heads, gqa_groups, dtype=torch_dtype)\n",
    "#             new_value = torch.cat(gqa_tensors_grouped)\n",
    "#             attentions_wts[name_wts] = new_value\n",
    "\n",
    "# # Process the biases\n",
    "# for name_bias in list(attentions_wts.keys()):\n",
    "#     if \"bias\" in name_bias:\n",
    "#         bias_tensor_to_process = attentions_wts[name_bias].clone()\n",
    "#         torch_dtype = bias_tensor_to_process.dtype\n",
    "        \n",
    "#         # Process k_proj biases\n",
    "#         if \"k_proj\" in name_bias:\n",
    "#             # Assumes biases can be split similarly, typically biases are 1D\n",
    "#             attn_heads = split_attention_to_heads(bias_tensor_to_process.unsqueeze(1), num_splits=num_heads)\n",
    "#             gqa_tensors_grouped = average_heads(attn_heads, gqa_groups, dtype=torch_dtype)\n",
    "#             new_key_bias = torch.cat(gqa_tensors_grouped).squeeze(1)  # Remove the added dimension\n",
    "#             attentions_wts[name_bias] = new_key_bias\n",
    "        \n",
    "#         # Process v_proj biases\n",
    "#         elif \"v_proj\" in name_bias:\n",
    "#             attn_heads = split_attention_to_heads(bias_tensor_to_process.unsqueeze(1), num_splits=num_heads)\n",
    "#             gqa_tensors_grouped = average_heads(attn_heads, gqa_groups, dtype=torch_dtype)\n",
    "#             new_value_bias = torch.cat(gqa_tensors_grouped).squeeze(1)  # Remove the added dimension\n",
    "#             attentions_wts[name_bias] = new_value_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "831e90ec-1334-4072-a238-f1f755e94a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the weights for Phi models\n",
    "\n",
    "for name_wts in list(attentions_wts.keys()):\n",
    "    if (\"qkv_proj\" in name_wts):\n",
    "        qkv_tensor = attentions_wts[name_wts].clone()\n",
    "        query_mha = qkv_tensor[0:3072, :]\n",
    "        key_mha = qkv_tensor[3072:6144, :]\n",
    "        value_mha = qkv_tensor[6144:9216, :]\n",
    "        torch_dtype = qkv_tensor.dtype\n",
    "\n",
    "        new_key = None\n",
    "        new_value = None\n",
    "        new_qkv_proj = None\n",
    "        for idx, tensor_to_convert in enumerate([key_mha, value_mha]):\n",
    "            attn_heads = split_attention_to_heads(tensor_to_convert, num_splits=num_heads)\n",
    "            gqa_tensors_grouped = average_heads(attn_heads, gqa_groups, dtype=torch_dtype)\n",
    "            if idx == 0:\n",
    "                new_key = torch.cat(gqa_tensors_grouped)\n",
    "            else:\n",
    "                new_value = torch.cat(gqa_tensors_grouped)\n",
    "        new_qkv_proj = torch.cat((query_mha, new_key, new_value), dim=0)\n",
    "        attentions_wts[name_wts] = new_qkv_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9648cded-afdf-45f6-bd39-0003c69af914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/drossini/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loading the model to test the answers with the dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "gqa_config = AutoConfig.from_pretrained(model_name, num_key_value_heads = 4)\n",
    "gqa_model = AutoModelForCausalLM.from_config(gqa_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b7d3000-a4c5-4185-8f2d-22d9cd563252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6083f6b-50e2-4dff-ab8f-f8844a65f8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gqa phi3 original wiegths but qkv_proj mean pooled\n",
    "\n",
    "gqa_model.load_state_dict(attentions_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab05c651-1ab3-47d6-b84a-8d86d5251d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gqa_model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdf188a9-63c4-4787-bf0d-8ba322b5cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gqa_model.save_pretrained(\"../home/drossini/models/phi3_mean_pooled/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
