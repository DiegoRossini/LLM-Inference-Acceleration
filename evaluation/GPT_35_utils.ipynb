{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "92c50026-ce0f-47ac-bc45-b3af28a374a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_values_df(model_answers_folder_path, \n",
        "                      cosine_sim_folder_path, \n",
        "                      start_day='2020-01-01', \n",
        "                      start_hour='00:00:00', \n",
        "                      end_day='2030-12-31', \n",
        "                      end_hour='23:59:59'):\n",
        "    \"\"\"\n",
        "    Concatenates DataFrames from multiple JSONL files within the specified time interval\n",
        "    and adds GPT-3.5 judgment and cosine similarity values to the model answers DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        model_answers_folder_path (str): Path to the folder containing model answers JSONL files.\n",
        "        cosine_sim_folder_path (str): Path to the folder containing cosine similarity JSONL files.\n",
        "        start_day (str): Start day of the time interval (default is '2020-01-01').\n",
        "        start_hour (str): Start hour of the time interval (default is '00:00:00').\n",
        "        end_day (str): End day of the time interval (default is '2030-12-31').\n",
        "        end_hour (str): End hour of the time interval (default is '23:59:59').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing all model answers along with cosine similarity values.\n",
        "    \"\"\"\n",
        "    # Concatenate DataFrames from model answers JSONL files\n",
        "    all_model_answers_df = concatenate_df(get_files_in_interval(model_answers_folder_path, start_day, end_day, start_hour, end_hour))\n",
        "    \n",
        "    # Concatenate DataFrames from cosine similarity JSONL files\n",
        "    all_cosine_sim_df = concatenate_df(get_files_in_interval(cosine_sim_folder_path, start_day, end_day, start_hour, end_hour))\n",
        "    \n",
        "    # Add cosine similarity column to model answers DataFrame\n",
        "    all_model_answers_df['cosine_similarity_between_answers'] = all_cosine_sim_df.iloc[:, -1]\n",
        "    \n",
        "    return all_model_answers_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "38e438dc-0f92-4688-a635-2517a7378e5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function fot concatenating DataFrames from multiple JSONL files into a single DataFrame\n",
        "def concatenate_df(jsonl_paths_list):\n",
        "    # Convert each JSONL file into a DataFrame and store them in a list\n",
        "    all_df = [convert_jsonl_into_df(jsonl_path) for jsonl_path in jsonl_paths_list]\n",
        "    \n",
        "    # Concatenate all DataFrames into a single DataFrame\n",
        "    concat_df = pd.concat(all_df)\n",
        "    \n",
        "    # Reset the index of the concatenated DataFrame\n",
        "    concat_df.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    # Return the concatenated DataFrame\n",
        "    return concat_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ae74b997-a6f9-40cb-952d-d2f0b61052a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to retrieve all files in a timestamp range\n",
        "def get_files_in_interval(folder_path, start_day, end_day, start_hour, end_hour):\n",
        "    # Convert start and end day strings to datetime objects\n",
        "    start_datetime = datetime.strptime(start_day + ' ' + start_hour, '%Y-%m-%d %H:%M:%S')\n",
        "    end_datetime = datetime.strptime(end_day + ' ' + end_hour, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Get all files in the folder\n",
        "    all_files = os.listdir(folder_path)\n",
        "\n",
        "    # Compile regex pattern to match date and time in file names\n",
        "    pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2}')\n",
        "\n",
        "    # Filter files based on the given interval\n",
        "    files_in_interval = []\n",
        "    for file in all_files:\n",
        "        # Extract date and time from file name using regex\n",
        "        match = re.search(pattern, file)\n",
        "        if match:\n",
        "            file_datetime = datetime.strptime(match.group(), '%Y-%m-%d_%H:%M:%S')\n",
        "\n",
        "            # Check if the file datetime is within the specified interval\n",
        "            if start_datetime <= file_datetime <= end_datetime:\n",
        "                files_in_interval.append(os.path.join(folder_path, file))\n",
        "\n",
        "    return files_in_interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d0b5a1d3-4931-4544-b326-ec9979290c5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to convert a jsonl file into a pandas DataFrame\n",
        "def convert_jsonl_into_df(jsonl_path):\n",
        "    lines = []\n",
        "    with open(jsonl_path) as f:\n",
        "        lines = f.read().splitlines()\n",
        "        \n",
        "    df_inter = pd.DataFrame(lines)\n",
        "    df_inter.columns = ['json_element']\n",
        "    # Decoding json object into a dictionary\n",
        "    df_inter['json_element'].apply(json.loads)\n",
        "    # Json normalize will convert any semi-structured json data into a flat table\n",
        "    df_final = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
        "\n",
        "    return df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "72f69be1-0760-472e-ae80-c32896f3f4c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def answers_comparison_GPT_35_turbo(deployment_name, client, df_before_GPT, model_name, dataset_name, experience_timestamps):\n",
        "\n",
        "    \"\"\"\n",
        "    Generate \"Yes\" or \"No\" answers after comparing meaning similarity of two given answers.\n",
        "\n",
        "    Args:\n",
        "        deployment_name: name of the model used for the answer comparison.\n",
        "        client: information about the AzureOpenAI client.\n",
        "        QA_result: a QA_Results_Comparator object that stocks all contexts, questions and answers of a dataset and the corresponding model' answers.\n",
        "        model_name: name of the model who gave the one of the two answers to compare.\n",
        "        \n",
        "    Returns:\n",
        "        A QA_Results_Comparator object where all the GPT 3.5 turbo answers are stocked on a new key:value pair along with the corresponding contexts, questions and the compared answers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Record start time for the entire cicle\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get jsonl experience path containing model_answers in order to save GPT judgments in streaming\n",
        "    start_day, start_hour, end_day, end_hour = experience_timestamps\n",
        "    directory = os.path.dirname(f\"./streamed_results/{simplified_model_name}/GPT_35_comparison/\")\n",
        "    # Create the directory if it doesn't already exist\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    gpt_35_judgment_jsonl_path = f\"./streamed_results/{simplified_model_name}/GPT_35_comparison/GPT_35_comparison_{start_day}_{start_hour}_{end_day}_{end_hour}_experience.jsonl\"\n",
        "    \n",
        "    # Initialization of the prompt template\n",
        "    prompt_template = (\n",
        "        \"Based on the provided context and the answers' relation to the question, \"\n",
        "        \"determine if the two answers mean the same thing.\"\n",
        "        \"If they do, respond just with 'Yes'. If they do not, respond just with 'No'.\\n\"\n",
        "        \"Context: {dataset_context}\\n\"\n",
        "        \"Question: {dataset_question}\\n\"\n",
        "        \"Answer from dataset: {dataset_answer}\\n\"\n",
        "        \"Answer from {model_name}: {model_answer}\\n\"\n",
        "        \"Judgment:\"\n",
        "    )\n",
        "\n",
        "    # Create the recording jsonl file\n",
        "    with open(gpt_35_judgment_jsonl_path, 'a+') as f:\n",
        "        \n",
        "        # For index corresponding to the rows in the dataset\n",
        "        for idx in range(len(df_before_GPT)):\n",
        "    \n",
        "            # Retriving all values\n",
        "            timestamp = df_before_GPT['timestamp'][idx]\n",
        "            dataset_context = df_before_GPT['dataset_context'][idx]\n",
        "            dataset_question = df_before_GPT['dataset_question'][idx]\n",
        "            dataset_answer = df_before_GPT['dataset_answer'][idx]\n",
        "            model_answer = df_before_GPT['model_answer'][idx]\n",
        "            n_input_tokens = df_before_GPT['n_input_token'][idx]\n",
        "            n_output_tokens = df_before_GPT['n_output_token'][idx]\n",
        "            output_generation_time = df_before_GPT['generation_time'][idx]\n",
        "    \n",
        "            # Formatting the prompt template\n",
        "            comparison_prompt = prompt_template.format(\n",
        "                dataset_context=dataset_context,\n",
        "                dataset_question=dataset_question,\n",
        "                dataset_answer=dataset_answer,\n",
        "                model_answer=model_answer,\n",
        "                model_name=model_name\n",
        "            )\n",
        "    \n",
        "            # Creating the message to pass as prompt to GPT 3.5 turbo\n",
        "            message = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a friendly chatbot who always work out its own judgment before rushing to a conclusion.\"\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": comparison_prompt},\n",
        "             ]\n",
        "    \n",
        "            # Checking if GPT-3.5 turbo consider both answer to be similar or not. Adding the result to the df\n",
        "            gpt_35_judgment = None\n",
        "            try:\n",
        "                response = client.chat.completions.create(model=deployment_name, messages=message, max_tokens=1000)\n",
        "                response_text = response.choices[0].message.content\n",
        "                if re.match(r'^Yes', response_text):\n",
        "                    gpt_35_judgment = \"Yes\"\n",
        "                    df_before_GPT.at[idx, 'GPT_35_judgment'] = \"Yes\"\n",
        "                else:\n",
        "                    gpt_35_judgment = \"No\"\n",
        "                    df_before_GPT.at[idx, 'GPT_35_judgment'] = \"No\"\n",
        "            except:\n",
        "                gpt_35_judgment = \"Error\"\n",
        "                df_before_GPT.at[idx, 'GPT_35_judgment'] = \"Error\"\n",
        "                print(\"Error\")\n",
        "    \n",
        "            # Collecting data for a single line in the jsonl\n",
        "            model_line_jsonl = {\n",
        "                 \"timestamp\":timestamp,\n",
        "                 \"generation_time\":output_generation_time,\n",
        "                 \"n_input_token\":str(n_input_tokens), \n",
        "                 \"n_output_token\":str(n_output_tokens), \n",
        "                 \"dataset_context\":dataset_context,\n",
        "                 \"dataset_question\":dataset_question, \n",
        "                 \"dataset_answer\":dataset_answer,\n",
        "                 \"model_answer\":model_answer,\n",
        "                 \"gpt_35_judgment\":gpt_35_judgment\n",
        "            }\n",
        "    \n",
        "            json.dump(model_line_jsonl, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "    # Record end time for the entire cicle\n",
        "    end_time = time.time()\n",
        "    total_judgment_time = end_time - start_time\n",
        "    print(f\"Total judgment time for GPT 3.5 turbo: {time_conversion(total_judgment_time)}\")\n",
        "\n",
        "    # Returning a df containing all informations\n",
        "    df_after_GPT = df_before_GPT\n",
        "    return df_before_GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "18982b56-d9d7-4461-a89e-60dfd233537d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to retrieve all files in a timestamp range\n",
        "def get_files_in_interval(folder_path, start_day, end_day, start_hour, end_hour):\n",
        "    # Convert start and end day strings to datetime objects\n",
        "    start_datetime = datetime.strptime(start_day + ' ' + start_hour, '%Y-%m-%d %H:%M:%S')\n",
        "    end_datetime = datetime.strptime(end_day + ' ' + end_hour, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Get all files in the folder\n",
        "    all_files = os.listdir(folder_path)\n",
        "\n",
        "    # Compile regex pattern to match date and time in file names\n",
        "    pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2}')\n",
        "\n",
        "    # Filter files based on the given interval\n",
        "    files_in_interval = []\n",
        "    for file in all_files:\n",
        "        # Extract date and time from file name using regex\n",
        "        match = re.search(pattern, file)\n",
        "        if match:\n",
        "            file_datetime = datetime.strptime(match.group(), '%Y-%m-%d_%H:%M:%S')\n",
        "\n",
        "            # Check if the file datetime is within the specified interval\n",
        "            if start_datetime <= file_datetime <= end_datetime:\n",
        "                files_in_interval.append(os.path.join(folder_path, file))\n",
        "\n",
        "    return files_in_interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5e9f1284-96f1-4d0b-bac3-15811c96ceee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function reads a file containing test specifics, extracts the start and end times, and returns them\n",
        "def get_start_end_times(tests_specifics_path):\n",
        "    # Open file and read lines\n",
        "    with open(tests_specifics_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Initialize variables\n",
        "    start_day, start_hour, end_day, end_hour = None, None, None, None\n",
        "\n",
        "    # Find end day and hour\n",
        "    for line in reversed(lines):\n",
        "        if line.startswith(\"Until\"):\n",
        "            end_day, end_hour = line.split(\" = \")[1].split()\n",
        "            break\n",
        "\n",
        "    # Find start day and hour\n",
        "    for line in reversed(lines):\n",
        "        if line.startswith(\"Test on\"):\n",
        "            start_day, start_hour = line.split(\" = \")[1].split()\n",
        "            break\n",
        "\n",
        "    # Return start and end times\n",
        "    return start_day, start_hour, end_day, end_hour\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "133c4c9c-9ab8-4dd5-a2ba-0dcff24b3ed4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_plots(start_day, end_day, start_hour, end_hour, simplified_model_name, df_final):\n",
        "    \"\"\"\n",
        "    Save plots related to the test experience.\n",
        "\n",
        "    Parameters:\n",
        "        start_day: Start day of the test.\n",
        "        end_day: End day of the test.\n",
        "        start_hour: Start hour of the test.\n",
        "        end_hour: End hour of the test.\n",
        "        simplified_model_name (str): Simplified name of the model.\n",
        "        df_final (pd.DataFrame): DataFrame containing the final data.\n",
        "    \"\"\"\n",
        "    # Create directory for saving test plots\n",
        "    if not os.path.exists(f'./streamed_results/{simplified_model_name}/{start_day}_{start_hour}_{end_day}_{end_hour}_experience/'):\n",
        "        os.makedirs(f'./streamed_results/{simplified_model_name}/{start_day}_{start_hour}_{end_day}_{end_hour}_experience/')\n",
        "\n",
        "    # Selecting the column for plotting\n",
        "    column = df_final['cosine_similarity_between_answers']\n",
        "\n",
        "    # Plotting the distribution\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(column, bins=30, color='skyblue', edgecolor='black')\n",
        "    plt.xlabel('Values')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Cosine Similarity Values')\n",
        "    plt.savefig(f'./streamed_results/{simplified_model_name}/{start_day}_{start_hour}_{end_day}_{end_hour}_experience/cos_sim_distribution.png')\n",
        "\n",
        "    # Filter data where GPT 3.5 judgment is \"Yes\" or \"No\"\n",
        "    yes_df = df_final[(df_final['GPT_35_judgment'] == \"Yes\")]\n",
        "    no_df = df_final[(df_final['GPT_35_judgment'] == \"No\")]\n",
        "\n",
        "    # Set up the matplotlib figure\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot the distribution of cosine similarity values when GPT 3.5 says \"Yes\"\n",
        "    sns.histplot(yes_df['cosine_similarity_between_answers'], color=\"blue\", label=\"Yes\", kde=True, element=\"step\", stat=\"density\")\n",
        "\n",
        "    # Plot the distribution of cosine similarity values when GPT 3.5 says \"No\"\n",
        "    sns.histplot(no_df['cosine_similarity_between_answers'], color=\"red\", label=\"No\", kde=True, element=\"step\", stat=\"density\")\n",
        "\n",
        "    # Add some plot aesthetics\n",
        "    plt.xlabel('Cosine Similarity')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Cosine Similarity Value Distributions for GPT 3.5 Judgments \"Yes\" and \"No\"')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'./streamed_results/{simplified_model_name}/{start_day}_{start_hour}_{end_day}_{end_hour}_experience/cos_sim_distrib_on_GPT35_data.png')\n",
        "\n",
        "    # Code for calculating number_of_outliers\n",
        "    no_df = df_final[df_final['GPT_35_judgment'] == \"No\"]\n",
        "    Q1 = no_df['cosine_similarity_between_answers'].quantile(0.25)\n",
        "    Q3 = no_df['cosine_similarity_between_answers'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = no_df[(no_df['cosine_similarity_between_answers'] > upper_bound) & (no_df['cosine_similarity_between_answers'] > 0.9)]\n",
        "    number_of_outliers = len(outliers)\n",
        "\n",
        "    # Calculate total number of judgments when GPT 3.5 says \"No\"\n",
        "    total_no_judgments = no_df.shape[0]\n",
        "\n",
        "    # Ratio between outliers over sim_cos 0.9 and total \"No\" answers by GPT 3.5\n",
        "    percentage = number_of_outliers * 100 / total_no_judgments\n",
        "    \n",
        "    # Draw the boxplot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.boxplot(x='GPT_35_judgment', y='cosine_similarity_between_answers', data=df_final)\n",
        "    plt.xlabel(f\"GPT-3.5 judgment\\nOutliers when GPT-3.5 says 'No' with Cosine Similarity > 0.9: {number_of_outliers}/{total_no_judgments} ({round(percentage,2)}%)\")\n",
        "    plt.ylabel(\"Cosine Similarity\")\n",
        "    plt.title('Distribution of Cosine Similarity by GPT-3.5 Judgment')\n",
        "    \n",
        "    # Annotate the plot with the information about outliers\n",
        "    # plt.text(0.5, 0.5, f\"Outliers when GPT_3.5 says 'No' with Cosine Similarity > 0.9: {number_of_outliers}/{total_no_judgments} ({round(percentage,2)}%)\",\n",
        "    #          fontsize=12, ha='center')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'./streamed_results/{simplified_model_name}/{start_day}_{start_hour}_{end_day}_{end_hour}_experience/cos_sim_outliers_with_respect_GPT35.png')\n",
        "\n",
        "\n",
        "    # Filter out rows with 'Error' in 'gpt_35_judgment' column\n",
        "    filtered_df = df_final[df_final['GPT_35_judgment'] != 'Error']\n",
        "    \n",
        "    # Define the slices\n",
        "    slices = [(0.7, 0.8), (0.8, 0.9), (0.9, 1)]\n",
        "    \n",
        "    # Create subplots\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    \n",
        "    # Loop through each slice and plot histogram\n",
        "    for i, (lower, upper) in enumerate(slices):\n",
        "        \n",
        "        # Filter out cosine similarity values for the slice\n",
        "        cosine_slice = filtered_df[(filtered_df['cosine_similarity_between_answers'] >= lower) & (filtered_df['cosine_similarity_between_answers'] < upper)]\n",
        "        \n",
        "        # Grouping the data by GPT-3.5 judgment\n",
        "        grouped_df = cosine_slice.groupby('GPT_35_judgment')['cosine_similarity_between_answers']\n",
        "        \n",
        "        # Plotting the distribution of cosine similarity for each GPT-3.5 judgment category\n",
        "        grouped_df.plot(kind='hist', alpha=0.5, bins=20, ax=axs[i], legend=True)\n",
        "        axs[i].set_xlabel('Cosine Similarity')\n",
        "        axs[i].set_ylabel('Frequency')\n",
        "        axs[i].set_title(f'Distribution of Cosine Similarity for {lower} <= cos sim < {upper}')\n",
        "        axs[i].axvline(x=lower, color='red', linestyle='--', linewidth=1, label=f'Cosine threshold ({lower})')\n",
        "        axs[i].axvline(x=upper, color='red', linestyle='--', linewidth=1, label=f'Cosine threshold ({upper})')\n",
        "        axs[i].legend()\n",
        "        axs[i].grid(True)\n",
        "    \n",
        "    plt.savefig(f'./streamed_results/{simplified_model_name}/{start_day}_{start_hour}_{end_day}_{end_hour}_experience/cos_sim_distrib_slices.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "97bd7288-465b-43f9-a7bf-d80399a75356",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for recording all model generation parameter\n",
        "def write_test_experience_score(tests_specifics_path, df_final):\n",
        "    with open(tests_specifics_filepath, \"a\", encoding='utf-8') as f:\n",
        "        f.write(f\"Model score: {get_model_performance(df_final)}\\n\")\n",
        "        f.write(\"_________________________________________________________________________________________________________________________\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cf8fbdf1-0752-4dcb-a265-0170475ca325",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate the model performance\n",
        "def get_model_performance(df_final):\n",
        "\n",
        "    # Calculate total number of samples\n",
        "    total_values = len(df_final)\n",
        "    \n",
        "    # Count the number of samples with high similarity\n",
        "    high_similarity_count = (df_final['cosine_similarity_between_answers'] >= 0.85).sum()\n",
        "    \n",
        "    # Count the number of low similarity samples with 'Yes' judgment\n",
        "    remaining_yes_count = df_final.loc[df_final['cosine_similarity_between_answers'] < 0.85, 'GPT_35_judgment'].eq('Yes').sum()\n",
        "    \n",
        "    # Calculate the performance score\n",
        "    score = (high_similarity_count + remaining_yes_count) / total_values\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "27161221-3ab9-48a4-88f4-9dc77c7d5725",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for visualizing all statistical pngs saved in a folder\n",
        "def visualize_images(folder_path):\n",
        "    # Get a list of all PNG files in the folder\n",
        "    image_files = glob.glob(folder_path + '/*.png')\n",
        "    \n",
        "    # Visualize each image\n",
        "    for img_path in image_files:\n",
        "\n",
        "        # Before calling show(), convert the image to 'RGB' if it has an alpha channel ('RGBA')\n",
        "        img = Image.open(img_path)\n",
        "        if img.mode == 'RGBA':\n",
        "            img = img.convert('RGB')\n",
        "        img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6907f677-1023-40b3-ae67-191239bbe9a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to convert time calculated just on seconds into time format days/hours/minutes/seconds\n",
        "def time_conversion(seconds):\n",
        "    \n",
        "    # Calculate days\n",
        "    days = int(seconds // (24 * 3600))\n",
        "    # Subtract days to find remaining seconds\n",
        "    seconds %= 24 * 3600\n",
        "    \n",
        "    # Calculate hours\n",
        "    hours = int(seconds // 3600)\n",
        "    # Subtract hours to find remaining seconds\n",
        "    seconds %= 3600\n",
        "    \n",
        "    # Calculate minutes\n",
        "    minutes = int(seconds // 60)\n",
        "    # Subtract minutes to find remaining seconds\n",
        "    seconds = round(seconds % 60, 2)  # Round seconds if necessary\n",
        "    \n",
        "    return f\"{days}:{hours}:{minutes}:{seconds}\""
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
