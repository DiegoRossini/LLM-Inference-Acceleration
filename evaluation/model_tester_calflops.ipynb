{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a90496-025f-4390-9cbb-913f5b82c74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Description  : \\n Version      : 1.0\\n Author       : MrYXJ\\n Mail         : yxj2017@gmail.com\\n Github       : https://github.com/MrYxJ\\n Date         : 2023-08-24 11:49:08\\n LastEditTime : 2023-09-03 11:38:11\\n Copyright (C) 2023 mryxj. All rights reserved.\\n '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "'''\n",
    " Description  : \n",
    " Version      : 1.0\n",
    " Author       : MrYXJ\n",
    " Mail         : yxj2017@gmail.com\n",
    " Github       : https://github.com/MrYxJ\n",
    " Date         : 2023-08-24 11:49:08\n",
    " LastEditTime : 2023-09-03 11:38:11\n",
    " Copyright (C) 2023 mryxj. All rights reserved.\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a1e81b-66f8-42ba-a96f-feff97de8c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/drossini/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os \n",
    "\n",
    "api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "login(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6d6807-57c9-4fb7-955e-7f60f87f9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = \"microsoft/Phi-3-mini-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3b8f3d2-3920-41a7-9f82-0b9ae08d80bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4c480d5a654a9c8d55b1f3df997fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model, trust_remote_code=True, use_fast=True, from_slow=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fdeb978-8bf1-4721-aa6a-0133f78db7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from calflops import calculate_flops\n",
    "\n",
    "# config = AutoConfig.from_pretrained(hf_model)\n",
    "# config.num_key_value_heads = 1\n",
    "# config.save_pretrained(model_path)  # Save the modified configuration to a specified path\n",
    "\n",
    "batch_size = 1\n",
    "max_seq_length = 128\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a365adeb-b0eb-4db1-9d5a-2802d03c5535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drossini/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2653: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  7.25 B  \n",
      "fwd MACs:                                                               914.83 GMACs\n",
      "fwd FLOPs:                                                              1.83 TFLOPS\n",
      "fwd+bwd MACs:                                                           2.74 TMACs\n",
      "fwd+bwd FLOPs:                                                          5.49 TFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "MistralForCausalLM(\n",
      "  7.25 B = 100% Params, 914.83 GMACs = 100% MACs, 1.83 TFLOPS = 50% FLOPs\n",
      "  (model): MistralModel(\n",
      "    7.11 B = 98.15% Params, 897.65 GMACs = 98.12% MACs, 1.8 TFLOPS = 49.06% FLOPs\n",
      "    (embed_tokens): Embedding(134.22 M = 1.85% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 32768, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        218.11 M = 3.01% Params, 28.05 GMACs = 3.07% MACs, 56.11 GFLOPS = 1.53% FLOPs\n",
      "        (self_attn): MistralAttention(\n",
      "          41.94 M = 0.58% Params, 5.5 GMACs = 0.6% MACs, 11.01 GFLOPS = 0.3% FLOPs\n",
      "          (q_proj): Linear(16.78 M = 0.23% Params, 2.15 GMACs = 0.23% MACs, 4.29 GFLOPS = 0.12% FLOPs, in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(4.19 M = 0.06% Params, 536.87 MMACs = 0.06% MACs, 1.07 GFLOPS = 0.03% FLOPs, in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(4.19 M = 0.06% Params, 536.87 MMACs = 0.06% MACs, 1.07 GFLOPS = 0.03% FLOPs, in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(16.78 M = 0.23% Params, 2.15 GMACs = 0.23% MACs, 4.29 GFLOPS = 0.12% FLOPs, in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          176.16 M = 2.43% Params, 22.55 GMACs = 2.46% MACs, 45.1 GFLOPS = 1.23% FLOPs\n",
      "          (gate_proj): Linear(58.72 M = 0.81% Params, 7.52 GMACs = 0.82% MACs, 15.03 GFLOPS = 0.41% FLOPs, in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(58.72 M = 0.81% Params, 7.52 GMACs = 0.82% MACs, 15.03 GFLOPS = 0.41% FLOPs, in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(58.72 M = 0.81% Params, 7.52 GMACs = 0.82% MACs, 15.03 GFLOPS = 0.41% FLOPs, in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 1.84 MFLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        (post_attention_layernorm): MistralRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "  )\n",
      "  (lm_head): Linear(134.22 M = 1.85% Params, 17.18 GMACs = 1.88% MACs, 34.36 GFLOPS = 0.94% FLOPs, in_features=4096, out_features=32768, bias=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "mistralai/Mistral-7B-Instruct-v0.3 FLOPs:1.83 TFLOPS   MACs:914.83 GMACs   Params:7.25 B \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flops, macs, params = calculate_flops(model=model,\n",
    "                                      input_shape=(batch_size, max_seq_length),\n",
    "                                      transformer_tokenizer=tokenizer\n",
    "                                     )\n",
    "print(f\"{hf_model} FLOPs:%s   MACs:%s   Params:%s \\n\" %(flops, macs, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4ea0f-8b84-4055-bdf4-660f3b5623e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
